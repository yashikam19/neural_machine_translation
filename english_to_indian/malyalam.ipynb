{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqTI7HJc3imX"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from collections import Counter\n",
        "import torch\n",
        "import torchtext\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "godExJ8w-53v"
      },
      "outputs": [],
      "source": [
        "!pip install torchtext==0.6.0 --quiet\n",
        "from torchtext.data import Field, BucketIterator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nENM1aZFVem7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a78ddc9c-3da3-40d9-a740-4845ec694a47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEVV5sneV2mE"
      },
      "outputs": [],
      "source": [
        "# Read JSON data from file\n",
        "with open('/gdrive/MyDrive/train_data1.json', 'r') as file:\n",
        "    data = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YRYpdQ-cTFe"
      },
      "outputs": [],
      "source": [
        "# Read JSON data from file\n",
        "with open('/gdrive/MyDrive/val_data1 (1).json', 'r') as file:\n",
        "    validation_data = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbbSk2oTBrqu"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "# Define the remove_punctuations function\n",
        "def remove_punctuations(sentence):\n",
        "    punctuations = list(string.punctuation)\n",
        "    punctuations.append('।')\n",
        "    punctuations.append('৷')\n",
        "    punctuations.append('’')\n",
        "    punctuations.append('‘')\n",
        "    cleaned = \"\"\n",
        "    for letter in sentence:\n",
        "        if letter not in punctuations:\n",
        "            cleaned += letter\n",
        "    return cleaned\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1VhqvkK8KUZ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Define a function to check if a sentence contains English words\n",
        "def contains_english_words(sentence):\n",
        "    return bool(re.search(r'[a-zA-Z]', sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uatShiZLWMGW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb161e97-8ab0-46ab-c69b-a1173d9301f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Language Pair: English-Malayalam\n",
            "  Data Type: Train\n"
          ]
        }
      ],
      "source": [
        "# Process JSON data\n",
        "source_sentences_train = []\n",
        "target_sentences_train = []\n",
        "len_malyalam = []\n",
        "len_english = []\n",
        "\n",
        "id_train = []\n",
        "\n",
        "for language_pair, language_data in data.items():\n",
        "    if(language_pair == \"English-Malayalam\"):\n",
        "      print(f\"Language Pair: {language_pair}\")\n",
        "      for data_type, data_entries in language_data.items():\n",
        "          print(f\"  Data Type: {data_type}\")\n",
        "          for entry_id, entry_data in data_entries.items():\n",
        "              source = entry_data[\"source\"].lower()\n",
        "              target = entry_data[\"target\"].lower()\n",
        "              source = remove_punctuations(source)\n",
        "              target = remove_punctuations(target)\n",
        "              if not contains_english_words(target):\n",
        "                    source_sentences_train.append(source)\n",
        "                    target_sentences_train.append(target)\n",
        "                    id_train.append(entry_id)\n",
        "                    len_malyalam.append(len(source.split(' ')))\n",
        "                    len_english.append(len(target.split(' ')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVf8B_zz-syB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63e49068-2492-4270-e540-3736b98f24e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum Length of malyalam Sentence: 107 words\n",
            "Maximum Length of English Sentence: 108 words\n"
          ]
        }
      ],
      "source": [
        "# Calculate the maximum length of malyalam and English sentences\n",
        "max_length_malyalam = max(len_malyalam)\n",
        "max_length_english = max(len_english)\n",
        "\n",
        "print(f\"Maximum Length of malyalam Sentence: {max_length_malyalam} words\")\n",
        "print(f\"Maximum Length of English Sentence: {max_length_english} words\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cpQZGso_JrZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc9dab0b-444f-4277-e962-d54043bfd9cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52562"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "len(target_sentences_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWatomKYc1f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3da821d-3cbc-478f-e707-07403df4d6fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Language Pair: English-Malayalam\n",
            "  Data Type: Validation\n"
          ]
        }
      ],
      "source": [
        "validation_source = []\n",
        "valid_id = []\n",
        "len_val=[]\n",
        "\n",
        "\n",
        "for language_pair, language_data in validation_data.items():\n",
        "    if(language_pair == \"English-Malayalam\"):\n",
        "      print(f\"Language Pair: {language_pair}\")\n",
        "      for data_type, data_entries in language_data.items():\n",
        "          print(f\"  Data Type: {data_type}\")\n",
        "          for entry_id, entry_data in data_entries.items():\n",
        "              source = entry_data[\"source\"].lower()\n",
        "              source = remove_punctuations(source)\n",
        "              validation_source.append(source)\n",
        "              valid_id.append(entry_id)\n",
        "              len_val.append(len(source.split(' ')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMaZS3YeObUV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35be9778-43d3-44ba-a660-d74b96d75d66"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7723"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "len(validation_source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IR3JdzR-DgTx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f09422e-32ec-4888-e3c4-d8571039c037"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "76\n"
          ]
        }
      ],
      "source": [
        "print(max(len_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy0fJ9dgSreI"
      },
      "outputs": [],
      "source": [
        "nlp_en = spacy.load('en_core_web_sm')\n",
        "# Tokenize English sentences\n",
        "tokenized_source_sentences = []\n",
        "for sentence in source_sentences_train:\n",
        "    doc = nlp_en(sentence)\n",
        "    tokens = [token.text for token in doc]\n",
        "    tokenized_source_sentences.append(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipibVT2fJFVC"
      },
      "outputs": [],
      "source": [
        "tokenized_validation_source = []\n",
        "for sentence in validation_source:\n",
        "    doc = nlp_en(sentence)\n",
        "    tokens = [token.text for token in doc]\n",
        "    tokenized_validation_source.append(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpuRRHTf0ce7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd2bc794-60b8-47de-9270-dc86a70b0683"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CH6S5tJIx1s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2594eac8-497d-4292-b585-1777f4eb8ead"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: indic-nlp-library in /usr/local/lib/python3.10/dist-packages (0.92)\n",
            "Requirement already satisfied: sphinx-argparse in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (0.4.0)\n",
            "Requirement already satisfied: sphinx-rtd-theme in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (1.3.0)\n",
            "Requirement already satisfied: morfessor in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (2.0.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (1.23.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2023.3.post1)\n",
            "Requirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library) (5.0.2)\n",
            "Requirement already satisfied: docutils<0.19 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->indic-nlp-library) (0.18.1)\n",
            "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->indic-nlp-library) (4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->indic-nlp-library) (1.16.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.7)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.5)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.4)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.1.9)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.6)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.1.2)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.16.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.12.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.7.13)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.31.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (23.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "pip install indic-nlp-library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1kY6K0TYfnL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07681b0a-2b4e-4e36-f958-aa6e50ab009e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Error loading indic_nltk: Package 'indic_nltk' not found\n",
            "[nltk_data]     in index\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# Download the Gujarati tokenizer\n",
        "nltk.download(\"indic_nltk\")\n",
        "\n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "\n",
        "\n",
        "# Tokenize malyalam sentences\n",
        "tokenized_target_sentences = []\n",
        "for sentence in target_sentences_train:\n",
        "    tokens = indic_tokenize.trivial_tokenize(sentence)\n",
        "    tokenized_target_sentences.append(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6F4avSE8VkuZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "137f0d8d-6b84-4f26-c6af-226f4c3899ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ആദ്യ വാരാന്ത്യത്തിൽ ഈ ചിത്രം 7 ദശലക്ഷം രൂപ 92000 യുഎസ് ഡോളർ നേടി\n",
            "['ആദ്യ', 'വാരാന്ത്യത്തിൽ', 'ഈ', 'ചിത്രം', '7', 'ദശലക്ഷം', 'രൂപ', '92000', 'യുഎസ്', 'ഡോളർ', 'നേടി']\n"
          ]
        }
      ],
      "source": [
        "print(target_sentences_train[5])\n",
        "print(tokenized_target_sentences[5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2F4gCLjN10F"
      },
      "outputs": [],
      "source": [
        "from torchtext.data import Dataset, Example, Field\n",
        "\n",
        "# Define your custom tokenizer function for English text\n",
        "def tokenize_english(text):\n",
        "    return text\n",
        "\n",
        "def tokenize_malyalam(text):\n",
        "    return text\n",
        "\n",
        "# Define your custom Datasets using tokenized sentences\n",
        "class CustomTranslationDataset(Dataset):\n",
        "    def __init__(self, source_sentences, target_sentences, source_field, target_field):\n",
        "        fields = [('source', source_field), ('target', target_field)]\n",
        "        examples = []\n",
        "        for src, tgt in zip(source_sentences, target_sentences):\n",
        "            examples.append(Example.fromlist([src, tgt], fields))\n",
        "        super().__init__(examples, fields)\n",
        "# Create Fields for English (source) and malyalam (target) text\n",
        "english = Field(tokenize=tokenize_english,\n",
        "                lower=True,\n",
        "                init_token=\"<sos>\",\n",
        "                eos_token=\"<eos>\")\n",
        "\n",
        "malyalam = Field(tokenize=tokenize_malyalam,\n",
        "              lower=True,\n",
        "              init_token=\"<sos>\",\n",
        "              eos_token=\"<eos>\")\n",
        "\n",
        "\n",
        "train_dataset = CustomTranslationDataset(tokenized_source_sentences, tokenized_target_sentences, english, malyalam)\n",
        "\n",
        "\n",
        "# Build vocabulary for the Fields with the same max_size\n",
        "english.build_vocab(train_dataset,max_size=30000, min_freq=3)\n",
        "malyalam.build_vocab(train_dataset,max_size=30000, min_freq=3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2iDGwTmfKVY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f48f49d0-1af5-4c75-9236-9dda094efef3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15784 25404\n"
          ]
        }
      ],
      "source": [
        "print(len(english.vocab), len(malyalam.vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyGOgGErijru"
      },
      "outputs": [],
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_iterator = BucketIterator(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    sort_within_batch=True,\n",
        "    sort_key=lambda x: len(x.source),\n",
        "    device=device\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13tSmhXAqU5l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb7df0e5-9a98-4e0a-d16d-eda0114629b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes torch.Size([6, 32]) torch.Size([13, 32])\n",
            "\n",
            "English -  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0') tensor([   91,   166,  1899,   157,   249,   268,   591,  2565,   431,    14,\n",
            "          479,    29,     0,    91,     8,  2436,   104,   168, 11559,  2150,\n",
            "          668,     8,   156, 14010,   264,   264,    67,  4170,   110,   395,\n",
            "           91,  9298], device='cuda:0') tensor([   8,   55,  384,   55,    6,   80, 1024,    0, 1828,   22,   52,   53,\n",
            "        1325,   10,    4,   80,   15,  429,   15,    5,   15,   17,   10,    4,\n",
            "        1089,  163,   28,   52,   52,   12,   23,   46], device='cuda:0') tensor([  616,    68,     6,  1112,   287,     4,     6,   280, 10265,   414,\n",
            "          483,   205,  2674,  1493,   678,   509,  9171,   451,    25,  2607,\n",
            "           25,  1075,   196,  9665,    10,     6,    92,  1819,   385,    81,\n",
            "          222,    12], device='cuda:0') tensor([  94, 2170,   17,  736,  546,  897, 2513, 3423,   14, 1920,  127, 2923,\n",
            "        1438,    0, 1549,  400,  190,  346,  210, 1237, 6919, 3744, 1928,  173,\n",
            "        5161, 1157,  512,  127, 2186,  225,   55, 3488], device='cuda:0') tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0')  Length -  6\n",
            "\n",
            "malyalam -  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0') tensor([ 1299,  6695,   219,    19,   374,  6078, 17578,  5664,  7526,  3973,\n",
            "           51,     0,     0,  9344,   568, 13877,     0,   113,     0,  8892,\n",
            "         9436,   141,    76,     0,     0, 22687,   267,    51,    51,    34,\n",
            "         1466,     0], device='cuda:0') tensor([ 1059,   144, 20741,     5,  3317,  1353,   414,  6747,  6920,  2960,\n",
            "         1603,   174, 16020,   142,     0,   932,   675,   346,   332,  5066,\n",
            "         6033,   247,  4425,   123,   861,   264,     0,   782,  1263, 14727,\n",
            "           81,     0], device='cuda:0') tensor([ 4266,   145,   438,  2477,   112,     3,  3077,  6747, 10225, 16097,\n",
            "          955,     3,  1352,  1400,     3,   395,   184,   190,     3,   559,\n",
            "           22,     0,     0, 11491,   674,  1966,   721,   192, 11589,   417,\n",
            "          142,    58], device='cuda:0') tensor([    3,   378,     3,   916,     3,     1,     3,     0,  7852,  3973,\n",
            "          192,     1,     3,   531,     1,     3,   136,   212,     1,     3,\n",
            "            3,     3, 16607,  3185,     3,     3,     3,  5977,     3,     3,\n",
            "         2271,  8960], device='cuda:0') tensor([    1,     3,     1,   517,     1,     1,     1,     0, 16632,     0,\n",
            "          232,     1,     1,     3,     1,     1,     3,   682,     1,     1,\n",
            "            1,     1,     3,     3,     1,     1,     1,     3,     1,     1,\n",
            "            3,     3], device='cuda:0') tensor([   1,    1,    1, 2288,    1,    1,    1, 3249,    3, 3450,   16,    1,\n",
            "           1,    1,    1,    1,    1,    3,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1], device='cuda:0') tensor([  1,   1,   1,   3,   1,   1,   1, 581,   1,  72,   3,   1,   1,   1,\n",
            "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "          1,   1,   1,   1], device='cuda:0') tensor([   1,    1,    1,    1,    1,    1,    1, 2320,    1, 4839,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1], device='cuda:0') tensor([ 1,  1,  1,  1,  1,  1,  1,  3,  1, 11,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
            "       device='cuda:0') tensor([    1,     1,     1,     1,     1,     1,     1,     1,     1, 19838,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1], device='cuda:0') tensor([  1,   1,   1,   1,   1,   1,   1,   1,   1, 650,   1,   1,   1,   1,\n",
            "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "          1,   1,   1,   1], device='cuda:0') tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')  Length -  13\n"
          ]
        }
      ],
      "source": [
        "count = 0\n",
        "for data in train_iterator:\n",
        "  if count < 1 :\n",
        "    print(\"Shapes\", data.source.shape, data.target.shape)\n",
        "    print()\n",
        "    print(\"English - \",*data.source, \" Length - \", len(data.source))\n",
        "    print()\n",
        "    print(\"malyalam - \",*data.target, \" Length - \", len(data.target))\n",
        "    temp_eng = data.source\n",
        "    temp_malyalam = data.target\n",
        "    count += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYf7MW1ErOAD"
      },
      "outputs": [],
      "source": [
        "temp_eng_idx = (temp_eng).cpu().detach().numpy()\n",
        "temp_malyalam_idx = (temp_malyalam).cpu().detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0euNdwKPDrP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cc52d1f-573d-41ac-e739-e00b191a3f99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(english.vocab.__dict__.keys())\n",
        "print(list(english.vocab.__dict__.values()))\n",
        "e = list(english.vocab.__dict__.values())\n",
        "for i in e:\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "It6E1yOtPMIk"
      },
      "outputs": [],
      "source": [
        "\n",
        "word_2_idx = dict(e[3])\n",
        "idx_2_word = {}\n",
        "for k,v in word_2_idx.items():\n",
        "  idx_2_word[v] = k\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-SfaqUyTu7F"
      },
      "outputs": [],
      "source": [
        "class EncoderLSTM(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
        "    super(EncoderLSTM, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # Number of layers in the lstm\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    # Regularization parameter\n",
        "    self.dropout = nn.Dropout(p)\n",
        "    self.tag = True\n",
        "\n",
        "\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "\n",
        "\n",
        "    self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "\n",
        "    outputs, (hidden_state, cell_state) = self.LSTM(embedding)\n",
        "\n",
        "    return hidden_state, cell_state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edjmxn7PURnn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4067182-1867-4e36-dcca-fc6b07df80f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EncoderLSTM(\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (embedding): Embedding(15784, 300)\n",
            "  (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "input_size_encoder = len(english.vocab)\n",
        "encoder_embedding_size = 300\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "encoder_dropout = 0.5\n",
        "\n",
        "encoder_lstm = EncoderLSTM(input_size_encoder, encoder_embedding_size,\n",
        "                           hidden_size, num_layers, encoder_dropout).to(device)\n",
        "print(encoder_lstm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sotub-aeU_eU"
      },
      "outputs": [],
      "source": [
        "class DecoderLSTM(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, num_layers, p, output_size):\n",
        "    super(DecoderLSTM, self).__init__()\n",
        "\n",
        "    # Size of the one hot vectors that will be the input to the encoder\n",
        "    #self.input_size = input_size\n",
        "\n",
        "    # Output size of the word embedding NN\n",
        "    #self.embedding_size = embedding_size\n",
        "\n",
        "    # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # Number of layers in the lstm\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    # Size of the one hot vectors that will be the output to the encoder (English Vocab Size)\n",
        "    self.output_size = output_size\n",
        "\n",
        "    # Regularization parameter\n",
        "    self.dropout = nn.Dropout(p)\n",
        "\n",
        "    # Shape --------------------> (5376, 300) [input size, embedding dims]\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "\n",
        "    # Shape -----------> (300, 2, 1024) [embedding dims, hidden size, num layers]\n",
        "    self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n",
        "\n",
        "    # Shape -----------> (1024, 4556) [embedding dims, hidden size, num layers]\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  # Shape of x (32) [batch_size]\n",
        "  def forward(self, x, hidden_state, cell_state):\n",
        "\n",
        "    # Shape of x (1, 32) [1, batch_size]\n",
        "    x = x.unsqueeze(0)\n",
        "\n",
        "    # Shape -----------> (1, 32, 300) [1, batch_size, embedding dims]\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "\n",
        "    # Shape --> outputs (1, 32, 1024) [1, batch_size , hidden_size]\n",
        "    # Shape --> (hs, cs) (2, 32, 1024) , (2, 32, 1024) [num_layers, batch_size size, hidden_size] (passing encoder's hs, cs - context vectors)\n",
        "    outputs, (hidden_state, cell_state) = self.LSTM(embedding, (hidden_state, cell_state))\n",
        "\n",
        "    # Shape --> predictions (1, 32, 4556) [ 1, batch_size , output_size]\n",
        "    predictions = self.fc(outputs)\n",
        "\n",
        "    # Shape --> predictions (32, 4556) [batch_size , output_size]\n",
        "    predictions = predictions.squeeze(0)\n",
        "\n",
        "    return predictions, hidden_state, cell_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeVmyCXJVHr2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c0c0e36-849c-4ee8-df1c-17ba78f6d901"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecoderLSTM(\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (embedding): Embedding(25404, 300)\n",
            "  (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
            "  (fc): Linear(in_features=1024, out_features=25404, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "input_size_decoder = len(malyalam.vocab)\n",
        "decoder_embedding_size = 300\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "decoder_dropout = 0.5\n",
        "output_size = len(malyalam.vocab)\n",
        "\n",
        "decoder_lstm = DecoderLSTM(input_size_decoder, decoder_embedding_size,\n",
        "                           hidden_size, num_layers, decoder_dropout, output_size).to(device)\n",
        "print(decoder_lstm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apEpFMW4tJP5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17ec108b-3874-4dfb-d62c-d928a9299dc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([25, 32])\n",
            "torch.Size([21, 32])\n",
            "tensor([ 5648,  1562,     0,     0,    44,  3549,  3784,  1824, 14294,   880,\n",
            "            5,     5, 23044, 23280,  5939,     0,  5571,   848,   481,     0,\n",
            "            0,     0,  1598,     0,    52,  5840,  2419,  2723,   176,     5,\n",
            "         2994,    41], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for batch in train_iterator:\n",
        "  print(batch.source.shape)\n",
        "  print(batch.target.shape)\n",
        "  break\n",
        "\n",
        "x = batch.target[1]\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoZWDGHBVPvH"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, Encoder_LSTM, Decoder_LSTM):\n",
        "    super(Seq2Seq, self).__init__()\n",
        "    self.Encoder_LSTM = Encoder_LSTM\n",
        "    self.Decoder_LSTM = Decoder_LSTM\n",
        "\n",
        "  def forward(self, source, target, tfr=0.5):\n",
        "    # Shape - Source : (10, 32) [(Sentence length German + some padding), Number of Sentences]\n",
        "    batch_size = source.shape[1]\n",
        "\n",
        "    # Shape - Source : (14, 32) [(Sentence length English + some padding), Number of Sentences]\n",
        "    target_len = target.shape[0]\n",
        "    target_vocab_size = len(malyalam.vocab)\n",
        "\n",
        "    # Shape --> outputs (14, 32, 5766)\n",
        "    outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "\n",
        "    # Shape --> (hs, cs) (2, 32, 1024) ,(2, 32, 1024) [num_layers, batch_size size, hidden_size] (contains encoder's hs, cs - context vectors)\n",
        "    hidden_state, cell_state = self.Encoder_LSTM(source)\n",
        "\n",
        "    # Shape of x (32 elements)\n",
        "    x = target[0] # Trigger token\n",
        "\n",
        "    for i in range(1, target_len):\n",
        "      # Shape --> output (32, 5766)\n",
        "      output, hidden_state, cell_state = self.Decoder_LSTM(x, hidden_state, cell_state)\n",
        "      outputs[i] = output\n",
        "      best_guess = output.argmax(1) # 0th dimension is batch size, 1st dimension is word embedding\n",
        "      x = target[i] if random.random() < tfr else best_guess # Either pass the next word correctly from the dataset or use the earlier predicted word\n",
        "\n",
        "    # Shape --> outputs (14, 32, 5766)\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqLXVTC1jOxN"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Hyperparameters\n",
        "\n",
        "learning_rate = 0.001\n",
        "step = 0\n",
        "\n",
        "model = Seq2Seq(encoder_lstm, decoder_lstm).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "pad_idx = malyalam.vocab.stoi[\"\"]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iw7RzYtRlYoC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1502a3aa-1564-4373-f758-5ba110aa013a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (Encoder_LSTM): EncoderLSTM(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (embedding): Embedding(15784, 300)\n",
              "    (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
              "  )\n",
              "  (Decoder_LSTM): DecoderLSTM(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (embedding): Embedding(25404, 300)\n",
              "    (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
              "    (fc): Linear(in_features=1024, out_features=25404, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvyzmuBFfzBq"
      },
      "outputs": [],
      "source": [
        "def translate_sentence(model, sentence, english, malyalam, device, max_length=108):\n",
        "    nlp_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "    if type(sentence) == str:\n",
        "        tokens = [token.text for token in nlp_en(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "    tokens.insert(0, english.init_token)\n",
        "    tokens.append(english.eos_token)\n",
        "    text_to_indices = [english.vocab.stoi[token] for token in tokens]\n",
        "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
        "\n",
        "    # Build encoder hidden, cell state\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.Encoder_LSTM(sentence_tensor)\n",
        "\n",
        "    outputs = [malyalam.vocab.stoi[\"\"]]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.Decoder_LSTM(previous_word, hidden, cell)\n",
        "            best_guess = output.argmax(1).item()\n",
        "\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        # Model predicts it's the end of the sentence\n",
        "        if output.argmax(1).item() == malyalam.vocab.stoi[\"\"]:\n",
        "            break\n",
        "\n",
        "    translated_sentence = [malyalam.vocab.itos[idx] for idx in outputs]\n",
        "    return translated_sentence[1:]\n",
        "\n",
        "def bleu(data, model, english, malyalam, device):\n",
        "    targets = []\n",
        "    outputs = []\n",
        "\n",
        "    for example in data:\n",
        "        src = vars(example)[\"source\"]\n",
        "        trg = vars(example)[\"target\"]\n",
        "\n",
        "        prediction = translate_sentence(model, src,english,malyalam, device)\n",
        "        prediction = prediction[:-1]  # remove  token\n",
        "\n",
        "        targets.append([trg])\n",
        "        outputs.append(prediction)\n",
        "\n",
        "    return bleu_score(outputs, targets)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEcjLf77QmOE"
      },
      "outputs": [],
      "source": [
        "def pred_trans(model, tokens, english, malyalam, device, max_length=108):\n",
        "    tokens.insert(0, english.init_token)\n",
        "    tokens.append(english.eos_token)\n",
        "    text_to_indices = [english.vocab.stoi[token] for token in tokens]\n",
        "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
        "\n",
        "    # Build encoder hidden, cell state\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.Encoder_LSTM(sentence_tensor)\n",
        "\n",
        "    outputs = [malyalam.vocab.stoi[\"\"]]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.Decoder_LSTM(previous_word, hidden, cell)\n",
        "            best_guess = output.argmax(1).item()\n",
        "\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        # Model predicts it's the end of the sentence\n",
        "        if output.argmax(1).item() == malyalam.vocab.stoi[\"\"]:\n",
        "            break\n",
        "\n",
        "    translated_sentence = [malyalam.vocab.itos[idx] for idx in outputs]\n",
        "    return translated_sentence[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCLkmq6kiDRj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c552b85-1962-4f94-8c3f-ab88e8d10db6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 1 / 10\n",
            "Translated example sentence 1: \n",
            " ['സിനിമയിൽ', 'ആരംഭിച്ചപ്പോള്\\u200d', 'ആരംഭിച്ചപ്പോള്\\u200d', 'ഗണേഷ്', 'പുരാവസ്തു', 'പുരാവസ്തു', 'പ്രാധാന്യമുള്ളതാണ്', 'ഹായ്', 'ഹായ്', 'പുറത്തിറക്കിയത്', 'ഗുളികകളും', 'ഗുളികകളും', 'ഗുളികകളും', 'തുടരുന്നത്', 'കൂടാന്\\u200d', 'ജലം', 'വിഷയമായി', 'കാര്യാലയം', 'മുകളിലായി', 'അവകാശം', 'ഡൗൺലോഡ്', 'ഉത്തർപ്രദേശ്', 'പട്ടേൽ', 'കടയില്\\u200d', 'ഹിൽസ്', 'വീശുന്ന', 'റോഷ്നി', 'റോഷ്നി', 'കൃഷിക്കും', 'കൃഷിക്കും', 'സുഗന്ധവ്യഞ്ജനത്തിന്\\u200dറെ', 'കൃഷിക്കും', 'ആശുപത്രിയിലെ', 'ശേഖരിച്ച്', 'കടയിലേക്ക്', 'കേരളം', 'കേരളം', 'നിറവേറ്റുന്നു', 'ഓർമ്മിപ്പിക്കാൻ', 'അർത്ഥമില്ല', 'സംരക്ഷിക്കാന്\\u200d', 'ഡാറ്റ', 'പറയട്ടെ', 'കഥയാണ്', 'ആക്ഷൻ', 'വെണ്ണയും', 'ആഗ്രഹിക്കുന്നുവെങ്കില്\\u200d', 'ഇടാന്\\u200d', 'അർപ്പിക്കാൻ', 'അർപ്പിക്കാൻ', 'അവസ്ഥയിൽ', 'ഞരമ്പിന്\\u200dറെ', 'കൃഷിക്കും', 'കൃഷിക്കും', 'സുഗന്ധവ്യഞ്ജനത്തിന്\\u200dറെ', 'കൃഷിക്കും', 'ആശുപത്രിയിലെ', 'ശേഖരിച്ച്', 'കടയിലേക്ക്', 'കേരളം', 'കേരളം', 'അടിക്കാൻ', 'അതിനനുസരിച്ച്', 'ഓർമ്മിപ്പിക്കാൻ', 'തീർത്ഥാടന', 'സംരക്ഷണത്തിനു', 'അർത്ഥമില്ല', 'ഗോവക്കാര്\\u200d', 'വാസ്തു', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന', 'എടുക്കുന്ന']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1643/1643 [05:07<00:00,  5.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken for epoch 1: 308.26 seconds\n",
            "Epoch_Loss - 3.9716806411743164\n",
            "\n",
            "Epoch - 2 / 10\n",
            "Translated example sentence 1: \n",
            " ['ഇവിടെ', 'ഏറ്റവും', '<eos>', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 1093/1643 [03:24<02:06,  4.34it/s]"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "epoch_loss = 0.0\n",
        "num_epochs = 10\n",
        "best_loss = 999999\n",
        "best_epoch = -1\n",
        "sentence1 = \"avoid alcohol and illicit drugs\"\n",
        "ts1  = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  start_time = time.time()\n",
        "  print(\"Epoch - {} / {}\".format(epoch+1, num_epochs))\n",
        "  model.eval()\n",
        "  translated_sentence1 = translate_sentence(model, sentence1, english, malyalam,device, max_length=108)\n",
        "  print(f\"Translated example sentence 1: \\n {translated_sentence1}\")\n",
        "  ts1.append(translated_sentence1)\n",
        "\n",
        "  model.train(True)\n",
        "  for batch_idx, batch in tqdm(enumerate(train_iterator), total=len(train_iterator)):\n",
        "    input = batch.source.to(device)\n",
        "    target = batch.target.to(device)\n",
        "\n",
        "    # Pass the input and target for model's forward method\n",
        "    output = model(input, target)\n",
        "    output = output[1:].reshape(-1, output.shape[2])\n",
        "    target = target[1:].reshape(-1)\n",
        "\n",
        "    # Clear the accumulating gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Calculate the loss value for every epoch\n",
        "    loss = criterion(output, target)\n",
        "\n",
        "    # Calculate the gradients for weights & biases using back-propagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip the gradient value is it exceeds > 1\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "    # Update the weights values using the gradients we calculated using bp\n",
        "    optimizer.step()\n",
        "    step += 1\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "  # Calculate the time taken for the epoch\n",
        "  end_time = time.time()\n",
        "  epoch_time = end_time - start_time\n",
        "  print(f\"Time taken for epoch {epoch + 1}: {epoch_time:.2f} seconds\")\n",
        "\n",
        "  print(\"Epoch_Loss - {}\".format(loss.item()))\n",
        "  print()\n",
        "\n",
        "print(epoch_loss / len(train_iterator))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiKHAHf-zaxz"
      },
      "outputs": [],
      "source": [
        "s = validation_source[10]\n",
        "translation = translate_sentence(model, s, english, malyalam, device, max_length=128)\n",
        "cleaned_tokens = [token for token in translation if token not in ['<eos>', '<pad>']]\n",
        "readable_translation = ' '.join(cleaned_tokens)\n",
        "print(readable_translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ob02tpz90mY6"
      },
      "outputs": [],
      "source": [
        "malyalam_pred = []\n",
        "\n",
        "for tokens in tokenized_validation_source:\n",
        "  translation = pred_trans(model, tokens, english, malyalam, device, max_length=114)\n",
        "  cleaned_tokens = [token for token in translation if token not in ['<eos>', '<pad>','।','৷']]\n",
        "  pred = ' '.join(cleaned_tokens)\n",
        "  print(pred)\n",
        "  malyalam_pred.append(pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCBWwSm53uhs"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "# Create a list of dictionaries where each dictionary represents a row\n",
        "data = [{'valid_id': valid_id[i], 'malyalam_pred': malyalam_pred[i]} for i in range(len(valid_id))]\n",
        "\n",
        "# Specify the CSV file path\n",
        "csv_file_path = '/gdrive/MyDrive/malyalam2.csv'\n",
        "\n",
        "# Define the column names\n",
        "fields = ['valid_id', 'malyalam_pred']\n",
        "\n",
        "# Write the data to the CSV file\n",
        "with open(csv_file_path, 'w', newline='') as csvfile:\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fields)\n",
        "    writer.writeheader()\n",
        "    writer.writerows(data)\n",
        "\n",
        "print(f'Saved predictions to {csv_file_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHshykMgUJp1"
      },
      "outputs": [],
      "source": [
        "print(len(malyalam_pred))\n",
        "print(len(validation_source))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}